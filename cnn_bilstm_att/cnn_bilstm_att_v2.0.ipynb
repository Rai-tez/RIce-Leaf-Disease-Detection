{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rrmGtOssK8G2",
   "metadata": {
    "id": "rrmGtOssK8G2"
   },
   "source": [
    "# Import and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1933b42a",
   "metadata": {
    "id": "1933b42a"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Reshape, Lambda, MultiHeadAttention, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Preprocessing\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2gray, rgb2hsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063a0e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#             SET FIRST IF USING LOCAL OR COLAB               #\n",
    "###############################################################\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "USING_LOCAL = True\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "if (USING_LOCAL):\n",
    "    # set gpu to be the one doing the computation\n",
    "    phy_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    print(phy_devices)\n",
    "    if phy_devices:\n",
    "        tf.config.experimental.set_memory_growth(phy_devices[0], True)\n",
    "        \n",
    "    # this folder must exist in your local where this ntbk is located\n",
    "    # the folder 'data' must contain the dir of your dataset\n",
    "#     root = '../cnn/my-first-cnn/data'\n",
    "    root = 'data'\n",
    "    \n",
    "    #Drei's root path to datasets\n",
    "    #root = 'D:/Andrei/Andrei/Prog Applications/datasets'\n",
    "else:\n",
    "    # mount google drive to get the dataset\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # set root path of the dataset\n",
    "    # create shortcut of the dataset to your own drive\n",
    "    # 'Shared with me' dir wont work\n",
    "    root = '/content/drive/skewl/Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EpTB8IGbLMg_",
   "metadata": {
    "id": "EpTB8IGbLMg_"
   },
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aJ9j_uwo2QUK",
   "metadata": {
    "id": "aJ9j_uwo2QUK"
   },
   "source": [
    "### Get datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "020e9e71",
   "metadata": {
    "id": "399f533b"
   },
   "outputs": [],
   "source": [
    "def get_train_X_and_Y(train_path, size=(224,224), batch_size=1):\n",
    "    train_batch = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=train_path, \n",
    "        image_size=size,\n",
    "        labels='inferred',\n",
    "        label_mode='int',\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        seed = 9\n",
    "    )\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for images, labels in train_batch.take(-1):\n",
    "        X.append(images.numpy()[0,:,:,:])\n",
    "        Y.append(labels.numpy()[0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def get_train_XY_and_valid(train_path, size=(224,224), batch_size=1, split=.15):\n",
    "    train_batch = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=train_path, \n",
    "        image_size=size,\n",
    "        labels='inferred',\n",
    "        label_mode='int',\n",
    "        shuffle=True,\n",
    "        validation_split=split,\n",
    "        batch_size=batch_size,\n",
    "        subset=\"training\",\n",
    "        seed = 9\n",
    "    )\n",
    "    \n",
    "    valid_batch = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=train_path, \n",
    "        image_size=size, \n",
    "        labels='inferred',\n",
    "        label_mode='int',\n",
    "        validation_split=split,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        subset='validation',\n",
    "        seed = 9\n",
    "    )\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for images, labels in train_batch.take(-1):\n",
    "        X.append(images.numpy()[0,:,:,:])\n",
    "        Y.append(labels.numpy()[0])\n",
    "        \n",
    "    return np.array(X), np.array(Y), valid_batch\n",
    "\n",
    "\n",
    "def get_test(test_path, size=(224,224), batch_size=1):\n",
    "    test_batch = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=test_path, \n",
    "        image_size=size, \n",
    "        labels='inferred',\n",
    "        label_mode='int',\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for images, labels in test_batch.take(-1):\n",
    "        X.append(images.numpy()[0,:,:,:])\n",
    "        Y.append(labels.numpy()[0])\n",
    "    return np.array(X), np.array(Y), test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2c7ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = get_train_X_and_Y(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2834d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_1xLp9El3q7Q",
   "metadata": {
    "id": "_1xLp9El3q7Q"
   },
   "source": [
    "### Plotting batch images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500817c1",
   "metadata": {
    "id": "500817c1"
   },
   "outputs": [],
   "source": [
    "def plotImagesWithBatch(batch):\n",
    "    imgs, labels = next(batch)\n",
    "    fig, axes = plt.subplots(1,10,figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(imgs, axes):\n",
    "        ax.imshow(img/255)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a90ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(12):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     image = X[i][0,:,:,:]\n",
    "#     plt.imshow(image.astype(\"uint8\"))\n",
    "#     plt.title(class_names[Y[i][0]])\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zd7bfR0a3xo_",
   "metadata": {
    "id": "Zd7bfR0a3xo_"
   },
   "source": [
    "### Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "T25LZs2LUzXn",
   "metadata": {
    "id": "T25LZs2LUzXn"
   },
   "outputs": [],
   "source": [
    "def ReshapeLayer(x):\n",
    "    shape = x.shape\n",
    "    reshape = Reshape((shape[1],shape[2]*shape[3]))(x)\n",
    "    return reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2UVpmmxCLwoo",
   "metadata": {
    "id": "2UVpmmxCLwoo"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3hJgBIaL1SV",
   "metadata": {
    "id": "s3hJgBIaL1SV"
   },
   "source": [
    "### Set Training Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28267243",
   "metadata": {
    "id": "28267243"
   },
   "outputs": [],
   "source": [
    "\n",
    "# dataset_path = f'{root}/CompletePreprocess_RiceDiseaseDataset'\n",
    "#dataset_path = f'{root}/PreprocessedRiceDiseaseDataset'\n",
    "#dataset_path = f'{root}/Rice diseases(Machine learning finals)/RiceDiseaseDataset'\n",
    "# dataset_path = f'{root}/PetImages_2000-500-800'\n",
    "# dataset_path = f'{root}/threshold with shadow removal 3'\n",
    "# dataset_path = f'{root}/_PlantVillage_Apple'\n",
    "# dataset_path = f'{root}/_RiceDiseaseDataset'\n",
    "dataset_path = f'{root}/_Wang_Nabawasan'\n",
    "\n",
    "train_path = f'{dataset_path}/training'\n",
    "\n",
    "#########################################################################\n",
    "#      MUST BE FALSE IF USING CompletePreprocess_RiceDiseaseDataset     #\n",
    "#########################################################################\n",
    "# True if using unpreprocessed data\n",
    "include_preprocessing = False\n",
    "\n",
    "#########################################################################\n",
    "#      MUST BE FALSE IF USING CompletePreprocess_RiceDiseaseDataset     #\n",
    "#########################################################################\n",
    "# False if the available folders are only Train and Testing\n",
    "dedicated_validation_folder = False\n",
    "\n",
    "if dedicated_validation_folder:\n",
    "    valid_path = f'{dataset_path}/validation'\n",
    "    test_path = f'{dataset_path}/testing'\n",
    "else:\n",
    "    test_path = f'{dataset_path}/testing'\n",
    "\n",
    "per_batch_size = 16\n",
    "size = (224, 224)\n",
    "\n",
    "class_names = ['BrownSpot', 'Healthy', 'Hispa', 'LeafBlast']\n",
    "# class_names = ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy']\n",
    "# class_names = ['dog', 'cat']\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qgNlh1PAK1S4",
   "metadata": {
    "id": "qgNlh1PAK1S4"
   },
   "outputs": [],
   "source": [
    "input_shape = (size[0], size[1], 3)\n",
    "learn_rate = 0.0001\n",
    "#learn_rate = 0.001\n",
    "# learn_rate = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oWQQtPSnL9Um",
   "metadata": {
    "id": "oWQQtPSnL9Um"
   },
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a0556fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################################################\n",
    "# #                       MobileNet PURE                        #\n",
    "# ###############################################################\n",
    "# def create_MobileNet_pure():\n",
    "#     #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "#     base_model=tf.keras.applications.MobileNet(input_shape=(224, 224, 3))\n",
    "    \n",
    "#     #Start of model sequence \n",
    "#     x = base_model.layers[-6].output\n",
    "#     preds = Dense(num_classes,activation='softmax')(x) #final layer with softmax activation\n",
    "    \n",
    "#     model=Model(inputs=base_model.input,outputs=preds)\n",
    "    \n",
    "#     l = -23\n",
    "#     for layer in model.layers[:l]:\n",
    "#         layer.trainable=False\n",
    "#     for layer in model.layers[l:]:\n",
    "#         layer.trainable=True\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=learn_rate), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "# #     model.compile(optimizer=Adam(learning_rate=learn_rate), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63098943",
   "metadata": {
    "id": "63098943"
   },
   "outputs": [],
   "source": [
    "# ###############################################################\n",
    "# #                   MobileNet with BiLSTM                     #\n",
    "# ###############################################################\n",
    "# def create_MobileNet_model():\n",
    "#     #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "#     base_model=tf.keras.applications.MobileNet(\n",
    "#         include_top=False, #will not include FC Layers\n",
    "#         input_shape=(224, 224, 3),\n",
    "#         weights='imagenet'\n",
    "#     )\n",
    "    \n",
    "#     #the application of tanh as an activation in BiLSTM is better than relu\n",
    "#     #as it is one of the requirements to use cuDNN kernels for faster training\n",
    "#     #activation[relu] = 8 seconds avg in training after 1st epoch\n",
    "#     #activation[tanh] = 4 seconds avg in training after 1st epoch \n",
    "#     forward_layer = LSTM(128, return_sequences=True)\n",
    "#     backward_layer = LSTM(128, activation='tanh', return_sequences=True, recurrent_dropout=0, go_backwards=True)\n",
    "    \n",
    "#     bilstm1 = Bidirectional(forward_layer, backward_layer = backward_layer)\n",
    "#     bilstm2 = Bidirectional(LSTM(64, activation='tanh',recurrent_dropout=0))\n",
    "    \n",
    "#     #Start of model sequence \n",
    "#     x = base_model.output\n",
    "#     x = Lambda(ReshapeLayer)(x)\n",
    "#     x = bilstm1(x)\n",
    "#     x = bilstm2(x)\n",
    "    \n",
    "#     preds = Dense(num_classes,activation='softmax')(x) #final layer with softmax activation\n",
    "    \n",
    "#     model=Model(inputs=base_model.input,outputs=preds)\n",
    "    \n",
    "#     l = -23\n",
    "#     for layer in model.layers[:l]:\n",
    "#         layer.trainable=False\n",
    "#     for layer in model.layers[l:]:\n",
    "#         layer.trainable=True\n",
    "\n",
    "# #     model.compile(optimizer=Adam(learning_rate=learn_rate), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "#     model.compile(optimizer=Adam(learning_rate=learn_rate), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "270a6057",
   "metadata": {
    "id": "270a6057"
   },
   "outputs": [],
   "source": [
    "###############################################################\n",
    "#            MobileNet with BiLSTM and Attention              #\n",
    "###############################################################\n",
    "\n",
    "def create_MobileNet_Attention():\n",
    "    #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "    base_model=tf.keras.applications.MobileNet(\n",
    "        include_top=False, #will not include FC Layers\n",
    "        input_shape=(224, 224, 3),\n",
    "        dropout=0.3,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "\n",
    "    attention = MultiHeadAttention(num_heads=2, key_dim=2)\n",
    "    \n",
    "    #the application of tanh as an activation in BiLSTM is better than relu\n",
    "    #as it is one of the requirements to use cuDNN kernels for faster training\n",
    "    #activation[relu] = 8 seconds avg in training after 1st epoch\n",
    "    #activation[tanh] = 4 seconds avg in training after 1st epoch \n",
    "    forward_layer = LSTM(128, return_sequences=True)\n",
    "    backward_layer = LSTM(128, activation='tanh', return_sequences=True, recurrent_dropout=0, go_backwards=True)\n",
    "    \n",
    "    bilstm1 = Bidirectional(forward_layer, backward_layer = backward_layer)\n",
    "    bilstm2 = Bidirectional(LSTM(128, activation='tanh',recurrent_dropout=0))\n",
    "\n",
    "    #Start of model sequence \n",
    "    x = base_model.output\n",
    "\n",
    "    x = attention(x, x, training=True)\n",
    "    \n",
    "    x = Lambda(ReshapeLayer)(x)\n",
    "    x = bilstm1(x)\n",
    "    x = bilstm2(x)\n",
    "    preds = Dense(num_classes,activation='softmax')(x) #final layer with softmax activation\n",
    "\n",
    "    model=Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "    l = -23\n",
    "    for layer in model.layers[:l]:\n",
    "        layer.trainable=False\n",
    "    for layer in model.layers[l:]:\n",
    "        layer.trainable=True\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learn_rate), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b48deb",
   "metadata": {
    "id": "07b48deb"
   },
   "source": [
    "### Usual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15eb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#      DO NOT RUN THIS WHEN DOING CROSS VALIDATION TRAINING     #\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c039c94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c039c94",
    "outputId": "b8f64317-8118-4d5b-c6ae-e70ae839e754"
   },
   "outputs": [],
   "source": [
    "# train_batch, valid_batch = get_train_and_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "910009ff",
   "metadata": {
    "id": "910009ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mn = create_MobileNet_model()\n",
    "# mn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "xZ8Xa4Av6Iab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "xZ8Xa4Av6Iab",
    "outputId": "a6109979-0ce5-43c5-d793-f2d534bcc2e7"
   },
   "outputs": [],
   "source": [
    "# plotImagesWithBatch(train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98bd2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9354e2",
   "metadata": {
    "id": "5a9354e2"
   },
   "outputs": [],
   "source": [
    "# model_history = mn.fit(x=train_batch, validation_data=valid_batch, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76dbb8",
   "metadata": {
    "id": "dc76dbb8"
   },
   "source": [
    "### 5-Fold Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2731b9b7",
   "metadata": {
    "id": "2731b9b7"
   },
   "outputs": [],
   "source": [
    "model_label = 'mnet_bilstm_att_rice_kfolded'\n",
    "\n",
    "# set early stopping criteria\n",
    "pat = 5 # this is the number of epochs with no improvment after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1, baseline=None)\n",
    "\n",
    "# define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
    "model_checkpoint = ModelCheckpoint(f'model_checkpoints/{model_label}.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "# to save the history of models\n",
    "csv_logger = CSVLogger(f'logs/{model_label}.log', separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c2abc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 840 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#save the model history in a list after fitting so that we can plot later\n",
    "model_history = []\n",
    "model_eval_score = []\n",
    "\n",
    "n_folds=5\n",
    "epochs=100\n",
    "batch_size=32\n",
    "\n",
    "loaded_model_file_path = 'model_checkpoints/'\n",
    "loaded_model_file_name = model_label\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=n_folds, shuffle=False)\n",
    "\n",
    "X, Y = get_train_X_and_Y(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c0a3aba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c0a3aba",
    "outputId": "d648ecfb-bd6d-4726-e1fb-eb7a2d943efa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Fold: 1\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 19s 391ms/step - loss: 1.0841 - accuracy: 0.4904 - val_loss: 2.0150 - val_accuracy: 0.3168\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.01504, saving model to model_checkpoints\\mnet_bilstm_att_rice_kfolded.h5\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 2s 131ms/step - loss: 0.5587 - accuracy: 0.7478 - val_loss: 2.4148 - val_accuracy: 0.3366\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.01504\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 0.2691 - accuracy: 0.8862 - val_loss: 2.6598 - val_accuracy: 0.3762\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.01504\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 3s 149ms/step - loss: 0.0816 - accuracy: 0.9790 - val_loss: 3.1337 - val_accuracy: 0.3762\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.01504\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 3s 178ms/step - loss: 0.0331 - accuracy: 0.9947 - val_loss: 2.8732 - val_accuracy: 0.4455\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.01504\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 3s 155ms/step - loss: 0.0124 - accuracy: 0.9965 - val_loss: 2.5496 - val_accuracy: 0.4851\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.01504\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Evaluating on Fold 1:\n",
      "6/6 [==============================] - 1s 220ms/step - loss: 2.8809 - accuracy: 0.3869\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold: 2\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 16s 347ms/step - loss: 1.1339 - accuracy: 0.4326 - val_loss: 1.9971 - val_accuracy: 0.3168\n",
      "\n",
      "Epoch 00001: val_loss improved from 2.01504 to 1.99712, saving model to model_checkpoints\\mnet_bilstm_att_rice_kfolded.h5\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.6540 - accuracy: 0.7110 - val_loss: 2.7053 - val_accuracy: 0.3168\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.99712\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.3429 - accuracy: 0.8687 - val_loss: 2.9336 - val_accuracy: 0.3168\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.99712\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 3s 181ms/step - loss: 0.1604 - accuracy: 0.9457 - val_loss: 2.8472 - val_accuracy: 0.3366\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.99712\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 4s 224ms/step - loss: 0.1357 - accuracy: 0.9510 - val_loss: 2.4880 - val_accuracy: 0.3069\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.99712\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 0.1265 - accuracy: 0.9510 - val_loss: 2.3352 - val_accuracy: 0.3861\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.99712\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Evaluating on Fold 2:\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 2.3215 - accuracy: 0.3333\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold: 3\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 17s 298ms/step - loss: 1.1907 - accuracy: 0.4238 - val_loss: 1.7150 - val_accuracy: 0.3168\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.99712 to 1.71502, saving model to model_checkpoints\\mnet_bilstm_att_rice_kfolded.h5\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 2s 135ms/step - loss: 0.7205 - accuracy: 0.6813 - val_loss: 2.0520 - val_accuracy: 0.3267\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.71502\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 0.3440 - accuracy: 0.8564 - val_loss: 1.7466 - val_accuracy: 0.4752\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.71502\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.1518 - accuracy: 0.9492 - val_loss: 1.1719 - val_accuracy: 0.6139\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.71502 to 1.17190, saving model to model_checkpoints\\mnet_bilstm_att_rice_kfolded.h5\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 3s 188ms/step - loss: 0.0798 - accuracy: 0.9737 - val_loss: 1.3451 - val_accuracy: 0.5545\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.17190\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 3s 158ms/step - loss: 0.0711 - accuracy: 0.9790 - val_loss: 1.8644 - val_accuracy: 0.5149\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.17190\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 3s 147ms/step - loss: 0.0221 - accuracy: 0.9947 - val_loss: 1.6534 - val_accuracy: 0.5842\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.17190\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 4s 215ms/step - loss: 0.0182 - accuracy: 0.9982 - val_loss: 1.6650 - val_accuracy: 0.5842\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.17190\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 3s 182ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.6936 - val_accuracy: 0.5941\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.17190\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Evaluating on Fold 3:\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 1.4452 - accuracy: 0.6310\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold: 4\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 15s 331ms/step - loss: 1.1244 - accuracy: 0.4483 - val_loss: 2.0719 - val_accuracy: 0.3168\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.17190\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 2s 134ms/step - loss: 0.6422 - accuracy: 0.7128 - val_loss: 2.4339 - val_accuracy: 0.3168\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.17190\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 2s 133ms/step - loss: 0.2857 - accuracy: 0.9002 - val_loss: 2.2893 - val_accuracy: 0.3663\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.17190\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 2s 132ms/step - loss: 0.0986 - accuracy: 0.9650 - val_loss: 2.1552 - val_accuracy: 0.4356\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.17190\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 0.0407 - accuracy: 0.9860 - val_loss: 1.9203 - val_accuracy: 0.4554\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.17190\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 2s 132ms/step - loss: 0.0463 - accuracy: 0.9842 - val_loss: 1.6772 - val_accuracy: 0.5347\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.17190\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 2s 132ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.5268 - val_accuracy: 0.5941\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.17190\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.5707 - val_accuracy: 0.6436\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.17190\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 2s 131ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.6290 - val_accuracy: 0.6436\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.17190\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 2s 131ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.6657 - val_accuracy: 0.6436\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.17190\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 2s 136ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.6874 - val_accuracy: 0.6337\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.17190\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.7004 - val_accuracy: 0.6436\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.17190\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "Evaluating on Fold 4:\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 1.4935 - accuracy: 0.6786\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold: 5\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 14s 251ms/step - loss: 1.1074 - accuracy: 0.4729 - val_loss: 2.3270 - val_accuracy: 0.2277\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.17190\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 0.6308 - accuracy: 0.7531 - val_loss: 2.6081 - val_accuracy: 0.2376\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.17190\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 2s 130ms/step - loss: 0.2984 - accuracy: 0.8949 - val_loss: 2.4854 - val_accuracy: 0.3465\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.17190\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 2s 128ms/step - loss: 0.0939 - accuracy: 0.9720 - val_loss: 2.2286 - val_accuracy: 0.4158\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.17190\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 0.0322 - accuracy: 0.9930 - val_loss: 2.5723 - val_accuracy: 0.3762\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.17190\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 3s 174ms/step - loss: 0.0135 - accuracy: 0.9982 - val_loss: 1.8372 - val_accuracy: 0.4554\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.17190\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.6371 - val_accuracy: 0.5149\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.17190\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.5508 - val_accuracy: 0.6040\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.17190\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 3s 161ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.5375 - val_accuracy: 0.6040\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.17190\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 3s 189ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.5343 - val_accuracy: 0.6436\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.17190\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 3s 162ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.5561 - val_accuracy: 0.6535\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.17190\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 3s 143ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.6027 - val_accuracy: 0.6634\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.17190\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 4s 202ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.6625 - val_accuracy: 0.6634\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.17190\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.7119 - val_accuracy: 0.6832\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.17190\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.7526 - val_accuracy: 0.6832\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.17190\n",
      "Epoch 00015: early stopping\n",
      "\n",
      "Evaluating on Fold 5:\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 1.6558 - accuracy: 0.6726\n",
      "====================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, valid in kfold.split(X, Y):\n",
    "    print(f\"Training on Fold: {fold_no}\")\n",
    "\n",
    "    model = create_MobileNet_Attention()\n",
    "\n",
    "    model_history.append(model.fit(\n",
    "        X[train], Y[train],\n",
    "        validation_split=.15,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping, model_checkpoint, csv_logger],\n",
    "        verbose=1,\n",
    "        shuffle = False\n",
    "    ))\n",
    "    \n",
    "    print(f\"\\nEvaluating on Fold {fold_no}:\")\n",
    "    \n",
    "    model_eval_score.append(model.evaluate(X[valid], Y[valid], batch_size=batch_size, verbose=1))\n",
    "    \n",
    "    print(\"=======\"*12, end=\"\\n\\n\\n\")\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CJzN_sU6MsbL",
   "metadata": {
    "id": "CJzN_sU6MsbL"
   },
   "source": [
    "# Prediction and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c635b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading h5 file or the model that has undergon cross validation\n",
    "saved_model_file_path = 'model_checkpoints/'\n",
    "saved_model_file_name = model_label\n",
    "saved_model = tf.keras.models.load_model(saved_model_file_path + saved_model_file_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83c6efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "655e19c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_images, test_labels, test_batch = get_test(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc2c5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_labels\n",
    "# y_test = test_labels.argmax(axis=1) # for categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48e6e6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 3s\n"
     ]
    }
   ],
   "source": [
    "y_pred = saved_model.predict(x=test_images, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f790fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ARGMAX = y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee8746f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'Apple___Apple_scab'),\n",
       " Text(0, 1.5, 'Apple___Black_rot'),\n",
       " Text(0, 2.5, 'Apple___Cedar_apple_rust'),\n",
       " Text(0, 3.5, 'Apple___healthy')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/\n",
    "#Confusion Matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_ARGMAX)\n",
    "ax = sb.heatmap(conf_matrix, annot=True, cmap='Blues')\n",
    "ax.set_title('Confusion Matrix\\n')\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "ax.xaxis.set_ticklabels(class_names)\n",
    "ax.yaxis.set_ticklabels(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69846954",
   "metadata": {
    "id": "69846954"
   },
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred_ARGMAX, average = None)\n",
    "accuracy = accuracy_score(y_test, y_pred_ARGMAX)\n",
    "recall = recall_score(y_test, y_pred_ARGMAX, average = None)\n",
    "f1 = f1_score(y_test, y_pred_ARGMAX, average = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "661a2485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Apple___Apple_scab</th>\n",
       "      <td>88.888889</td>\n",
       "      <td>17.777778</td>\n",
       "      <td>29.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple___Black_rot</th>\n",
       "      <td>33.720930</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple___Cedar_apple_rust</th>\n",
       "      <td>33.333333</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>7.843137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple___healthy</th>\n",
       "      <td>31.944444</td>\n",
       "      <td>25.555556</td>\n",
       "      <td>28.395062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Precision     Recall         F1\n",
       "Apple___Apple_scab        88.888889  17.777778  29.629630\n",
       "Apple___Black_rot         33.720930  96.666667  50.000000\n",
       "Apple___Cedar_apple_rust  33.333333   4.444444   7.843137\n",
       "Apple___healthy           31.944444  25.555556  28.395062"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print('Precision: %.3f' % precision)\n",
    "#print('Accuracy: %.3f' % accuracy)\n",
    "#print('Recall: %.3f' % recall)\n",
    "#print('F1-Score: %.3f' % f1)\n",
    "format = {}\n",
    "\n",
    "for n in range(len(class_names)):\n",
    "    format[class_names[n]] = [precision[n]* 100, recall[n]* 100, f1[n]* 100]\n",
    "    \n",
    "df = pd.DataFrame(format, ['Precision', 'Recall', 'F1']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ad07087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.111\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.3f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "788ac5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 8s 14ms/step - loss: 2.2127 - accuracy: 0.3611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2127037048339844, 0.3611111044883728]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model.evaluate(x=test_batch, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d4959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "cnn-bilstm-att.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
